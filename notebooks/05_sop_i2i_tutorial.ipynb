{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3ea1df",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial: Fine-tuning MAE on SOP for Image-to-Image Retrieval\n",
    "\n",
    "In this tutorial, we will fine-tune a **Masked Autoencoder (MAE)** on the **Stanford Online Products (SOP)** dataset for the Image-to-Image (I2I) retrieval task.\n",
    "\n",
    "**Goal**: Train a model that takes a query image and retrieves the same product from a gallery of images (different views/conditions).\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Task**: Image-to-Image (I2I) Retrieval\n",
    "- **Model**: `facebook/vit-mae-base` (Vision Transformer trained with Masked Autoencoding)\n",
    "- **Loss**: InfoNCE (Contrastive Loss)\n",
    "- **Dataset**: Stanford Online Products (SOP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.exists(\"vembed-factory\"):\n",
    "    os.chdir(\"vembed-factory\")\n",
    "elif os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Install dependencies if needed\n",
    "# !pip install -e \".[all]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0acc13",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We use the Stanford Online Products (SOP) dataset.\n",
    "For this tutorial, we assume you have converted the SOP dataset into JSONL format suitable for `vembed-factory`.\n",
    "\n",
    "**Expected Format (JSONL)**:\n",
    "```json\n",
    "{\"query_image\": \"path/to/img1.jpg\", \"positive\": \"path/to/img2.jpg\", \"class_id\": 123}\n",
    "```\n",
    "- `query_image`: The anchor image.\n",
    "- `positive`: A positive sample (same product).\n",
    "- `class_id`: Product ID (used for evaluation).\n",
    "\n",
    "If you don't have the data, please refer to `benchmark/bench_datasets/sop.py` for processing logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = \"data/sop_i2i/train.jsonl\"\n",
    "VAL_DATA_PATH = \"data/sop_i2i/val.jsonl\"\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"Data not found at {DATA_PATH}\")\n",
    "    print(\"Please ensure you have prepared the SOP dataset.\")\n",
    "else:\n",
    "    print(f\"Found training data: {DATA_PATH}\")\n",
    "    # Preview data\n",
    "    !head -n 2 {DATA_PATH}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f8947",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "We will use `vembed.Trainer` to fine-tune the MAE model.\n",
    "We specify `retrieval_mode=\"i2i\"` to indicate Image-to-Image training (Siamese Image Encoders).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1889d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vembed import Trainer\n",
    "\n",
    "# Initialize Trainer with MAE model\n",
    "# mode=\"custom\" tells the factory to use the AutoModel backend\n",
    "trainer = Trainer(\n",
    "    model_name=\"facebook/vit-mae-base\",\n",
    "    mode=\"custom\",\n",
    "    output_dir=\"experiments/output_mae_i2i_notebook\"\n",
    ")\n",
    "\n",
    "# Start training\n",
    "# We use a small number of epochs for demonstration\n",
    "if os.path.exists(DATA_PATH):\n",
    "    trainer.train(\n",
    "        data_path=DATA_PATH,\n",
    "        val_data_path=VAL_DATA_PATH if os.path.exists(VAL_DATA_PATH) else None,\n",
    "        epochs=3,\n",
    "        batch_size=64,\n",
    "        learning_rate=5e-5,\n",
    "        retrieval_mode=\"i2i\",\n",
    "        use_gradient_cache=True,  # Enable gradient cache for memory efficiency\n",
    "        save_steps=500\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping training (no data).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f987c",
   "metadata": {},
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "After training, we can evaluate the model on the SOP test set.\n",
    "We provide a benchmark script `benchmark/compare_sop_before_after.sh` that compares the pre-trained MAE vs. our fine-tuned MAE.\n",
    "\n",
    "We can also test Matryoshka Representation Learning (MRL) performance by passing `mrl_dims`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dddccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark comparison script\n",
    "# This script runs the 'sop' benchmark pipeline and calculates Recall@K\n",
    "\n",
    "# Make sure the script is executable\n",
    "!chmod +x benchmark/compare_sop_before_after.sh\n",
    "\n",
    "# Run comparison (assuming SOP raw data is in data/stanford_online_products)\n",
    "# ./benchmark/compare_sop_before_after.sh <BEFORE_MODEL> <AFTER_MODEL> [SIMILARITY_MODE] [BATCH_SIZE] [TOPK] [MRL_DIMS]\n",
    "\n",
    "if os.path.exists(\"data/stanford_online_products\"):\n",
    "    !./benchmark/compare_sop_before_after.sh \\\n",
    "        facebook/vit-mae-base \\\n",
    "        experiments/output_mae_i2i_notebook/checkpoint-epoch-3 \\\n",
    "        cosine \\\n",
    "        64 \\\n",
    "        100 \\\n",
    "        \"768 512 256\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166f8ce",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Let's visualize the retrieval results!\n",
    "We will randomly select a few query images from the test set and show their Top-5 retrieved results using the embeddings we just computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c005351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Add project root to path\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    sys.path.append(\"..\")\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "# Import dataset loader from vembed-factory\n",
    "try:\n",
    "    from benchmark.bench_datasets.sop import _load_sop_entries\n",
    "except ImportError:\n",
    "    # Fallback if running from a different context\n",
    "    sys.path.append(os.path.abspath(\"..\"))\n",
    "    from benchmark.bench_datasets.sop import _load_sop_entries\n",
    "\n",
    "# Config\n",
    "SOP_ROOT = \"data/stanford_online_products\"\n",
    "EMB_DIR = \"experiments/benchmark_output_sop_compare/after\"\n",
    "\n",
    "def visualize_results():\n",
    "    if not os.path.exists(os.path.join(EMB_DIR, \"test_query_embeddings.npy\")):\n",
    "        print(f\"Embeddings not found in {EMB_DIR}. Please run the benchmark step above.\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading embeddings...\")\n",
    "    q_emb = np.load(os.path.join(EMB_DIR, \"test_query_embeddings.npy\"))\n",
    "    d_emb = np.load(os.path.join(EMB_DIR, \"test_doc_embeddings.npy\"))\n",
    "\n",
    "    q_norm = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)\n",
    "    d_norm = d_emb / (np.linalg.norm(d_emb, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    print(\"Loading SOP dataset index...\")\n",
    "    try:\n",
    "        entries, class_ids = _load_sop_entries(SOP_ROOT, \"test\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"SOP dataset index not found in {SOP_ROOT}. Cannot visualize images.\")\n",
    "        return\n",
    "\n",
    "    if len(entries) == 0:\n",
    "        print(\"Dataset is empty.\")\n",
    "        return\n",
    "\n",
    "    # Sanity Check\n",
    "    if len(entries) != len(q_emb):\n",
    "        print(f\"Warning: Dataset size ({len(entries)}) != Embedding size ({len(q_emb)}).\")\n",
    "        print(\"Using the minimum size for safety.\")\n",
    "        min_len = min(len(entries), len(q_emb))\n",
    "        entries = entries[:min_len]\n",
    "        q_norm = q_norm[:min_len]\n",
    "        d_norm = d_norm[:min_len]\n",
    "\n",
    "    def show_retrieval(idx, topk=5):\n",
    "        query_vec = q_norm[idx]\n",
    "\n",
    "        # Compute scores: (1, D) @ (N, D).T -> (1, N)\n",
    "        scores = query_vec @ d_norm.T\n",
    "\n",
    "        # Get Top-K indices\n",
    "        top_indices = np.argsort(scores)[::-1][:topk]\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, topk + 1, figsize=(15, 3))\n",
    "\n",
    "        # Query Image\n",
    "        q_path = entries[idx][\"query_image\"]\n",
    "        try:\n",
    "            img = Image.open(q_path)\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f\"Query\\nClass: {entries[idx]['class_id']}\")\n",
    "            axes[0].axis(\"off\")\n",
    "            # Add a colored border to query\n",
    "            for spine in axes[0].spines.values():\n",
    "                spine.set_edgecolor('blue')\n",
    "                spine.set_linewidth(2)\n",
    "        except Exception as e:\n",
    "            axes[0].text(0.5, 0.5, \"Img Not Found\", ha=\"center\")\n",
    "            print(f\"Error loading {q_path}: {e}\")\n",
    "\n",
    "        # Results\n",
    "        for i, res_idx in enumerate(top_indices):\n",
    "            res_path = entries[res_idx][\"positive\"]\n",
    "            score = scores[res_idx]\n",
    "            is_same_class = (entries[idx][\"class_id\"] == entries[res_idx][\"class_id\"])\n",
    "\n",
    "            ax = axes[i+1]\n",
    "            try:\n",
    "                img = Image.open(res_path)\n",
    "                ax.imshow(img)\n",
    "\n",
    "                # Title color based on correctness\n",
    "                color = \"green\" if is_same_class else \"red\"\n",
    "                title = f\"Rank {i+1}\\n{score:.3f}\"\n",
    "                ax.set_title(title, color=color, fontweight=\"bold\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            except Exception:\n",
    "                ax.text(0.5, 0.5, \"Img Not Found\", ha=\"center\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Visualizing random samples...\")\n",
    "    indices = np.random.choice(len(entries), 3, replace=False)\n",
    "    for idx in indices:\n",
    "        show_retrieval(idx, topk=5)\n",
    "\n",
    "    # Execute\n",
    "    visualize_results()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
