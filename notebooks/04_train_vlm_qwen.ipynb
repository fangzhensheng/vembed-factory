{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 4. Training Qwen-VL (VLM Embedding)\n",
    "\n",
    "This notebook demonstrates how to fine-tune a Large Vision-Language Model (Qwen-VL) for embedding tasks.\n",
    "This requires significantly more GPU memory than CLIP training.\n",
    "\n",
    "Ensure you have run `01_setup_and_data.ipynb` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.exists(\"vembed-factory\"):\n",
    "    os.chdir(\"vembed-factory\")\n",
    "elif os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(f\"Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Data Paths\n",
    "if os.path.exists(\"data/flickr30k/train.jsonl\"):\n",
    "    DATA_PATH = \"data/flickr30k/train.jsonl\"\n",
    "    IMAGE_ROOT = \"data/flickr30k\"\n",
    "    VAL_DATA_PATH = \"data/flickr30k/val.jsonl\"\n",
    "else:\n",
    "    DATA_PATH = \"data/dummy/train.jsonl\"\n",
    "    IMAGE_ROOT = \"data/dummy\"\n",
    "    VAL_DATA_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": "## Configuration\n\n- **Model**: `Qwen3-VL-Embedding-2B` \n- **Config**: Uses `examples/qwen3_2b_train.yaml` (standardized)\n- **Method**: LoRA + FlashAttention + Gradient Cache + MRL\n- **Memory Optimization**: Gradient Cache enabled with optimal chunk sizes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": "# Training Qwen3-VL using standardized config\n# Note: Gradient cache is optimally configured in qwen3_2b_train.yaml\n\n!python run.py examples/qwen3_2b_train.yaml \\\n    --data_path $DATA_PATH \\\n    --val_data_path \"$VAL_DATA_PATH\" \\\n    --image_root \"$IMAGE_ROOT\" \\\n    --config_override \\\n        output_dir=experiments/output_qwen_vl \\\n        epochs=1 \\\n        batch_size=2"
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(VAL_DATA_PATH):\n",
    "    !python benchmark/run.py flickr30k \\\n",
    "        --model_path experiments/output_qwen_vl/checkpoint-epoch-1 \\\n",
    "        --flickr_root $IMAGE_ROOT \\\n",
    "        --output_dir experiments/eval_results_qwen \\\n",
    "        --batch_size 16 \\\n",
    "        --encoder_mode qwen3_vl \\\n",
    "        --flickr_split val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}