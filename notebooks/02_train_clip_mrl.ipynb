{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 2. Training CLIP with MRL\n",
    "\n",
    "This notebook demonstrates how to fine-tune a CLIP model using **Matryoshka Representation Learning (MRL)**.\n",
    "Ensure you have run `01_setup_and_data.ipynb` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.exists(\"vembed-factory\"):\n",
    "    os.chdir(\"vembed-factory\")\n",
    "elif os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(f\"Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Data Paths\n",
    "if os.path.exists(\"data/flickr30k/train.jsonl\"):\n",
    "    DATA_PATH = \"data/flickr30k/train.jsonl\"\n",
    "    IMAGE_ROOT = \"data/flickr30k\"\n",
    "    VAL_DATA_PATH = \"data/flickr30k/val.jsonl\"\n",
    "else:\n",
    "    DATA_PATH = \"data/dummy/train.jsonl\"\n",
    "    IMAGE_ROOT = \"data/dummy\"\n",
    "    VAL_DATA_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "We use `vembed/cli.py` (via `python -m vembed.cli`) to launch training.\n",
    "- `model_type`: clip\n",
    "- `use_mrl`: Enable Matryoshka Loss\n",
    "- `mrl_dims`: [512, 256, 128] (Default for CLIP preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m vembed.cli \\\n",
    "    --model_type clip \\\n",
    "    --model_name \"openai/clip-vit-base-patch32\" \\\n",
    "    --data_path $DATA_PATH \\\n",
    "    --output_dir experiments/output_clip_mrl \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 32 \\\n",
    "    --use_mrl \\\n",
    "    --config_override image_root=\"$IMAGE_ROOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We use the benchmark script to evaluate the MRL-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(VAL_DATA_PATH):\n",
    "    # Evaluate on the validation set using MRL\n",
    "    !python benchmark/run.py flickr30k \\\n",
    "        --model_path experiments/output_clip_mrl/checkpoint-epoch-3 \\\n",
    "        --flickr_root $IMAGE_ROOT \\\n",
    "        --output_dir experiments/eval_results_mrl \\\n",
    "        --batch_size 64 \\\n",
    "        --encoder_mode clip \\\n",
    "        --flickr_split val \\\n",
    "        --mrl_dims 512 256 128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
