# CLIP ViT-B/32 with CoSENT loss
# Circle loss variant for normalized embeddings

model_name: openai/clip-vit-base-patch32
use_lora: true
loss_type: cosent

output_dir: experiments/output_clip_cosent
epochs: 5
batch_size: 128
learning_rate: 2.0e-5

data_path: data/train.jsonl
val_data_path: data/val.jsonl
image_root: data/images
