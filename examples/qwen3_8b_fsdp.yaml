# Qwen3-VL-Embedding-8B Training Configuration with FSDP

# Model
model_name: "Qwen/Qwen3-VL-Embedding-8B"
encoder_mode: "qwen3_vl"
attn_implementation: "flash_attention_2"
torch_dtype: "bfloat16"

# Data
data_path: "data/train.jsonl"
val_data_path: "data/val.jsonl"
image_root: "data/images"

# Training
output_dir: "experiments/output_qwen3_8b_fsdp"
epochs: 3
batch_size: 64
learning_rate: 2.0e-5
loss_type: "infonce"

# Memory Optimization with FSDP
# FSDP: Shards model parameters, gradients, and optimizer states across GPUs
# This enables training of 8B+ models on multi-GPU setups
use_fsdp: true
gradient_checkpointing: true
use_lora: true

# Multi-Resolution Learning (reduced dims for FSDP memory efficiency)
use_mrl: true
mrl_dims: [1024]
