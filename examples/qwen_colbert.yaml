# Qwen2-7B + ColBERT for text-to-text retrieval
# Late-interaction loss with token-level embeddings from VLM

model_name: Qwen/Qwen2-7B-Instruct
use_lora: true
retrieval_mode: t2t

# ColBERT: MaxSim-based late-interaction scoring
# VLM outputs [B, seq_len, D] with L2-normalized tokens
loss_type: colbert
pooling_method: none
projection_dim: 128

# Data
data_path: data/train.jsonl
val_data_path: data/val.jsonl
image_root: data/images
output_dir: experiments/output_qwen_colbert

# Training: gradient cache for handling large LLM
epochs: 3
batch_size: 64
learning_rate: 5.0e-05
use_gradient_cache: true
gradient_cache_chunk_size: 4

logging_steps: 10
save_steps: 500
